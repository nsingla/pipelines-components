# PIPELINE DEFINITION
# Name: evaluate-yoda-model
# Inputs:
#    add_bos_token: bool [Default: True]
#    batch_size: int [Default: 1.0]
#    custom_translation_dataset: system.Dataset
#    dtype: str [Default: 'auto']
#    gpu_memory_utilization: float [Default: 0.8]
#    include_classification_tasks: bool [Default: True]
#    include_summarization_tasks: bool [Default: True]
#    limit: int
#    log_prompts: bool [Default: True]
#    lora_adapter: system.Model
#    max_batch_size: int
#    max_model_len: int [Default: 4096.0]
#    model_path: str
#    verbosity: str [Default: 'INFO']
# Outputs:
#    output_metrics: system.Metrics
#    output_prompts: system.Artifact
#    output_results: system.Artifact
components:
  comp-evaluate-yoda-model:
    executorLabel: exec-evaluate-yoda-model
    inputDefinitions:
      artifacts:
        custom_translation_dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
          isOptional: true
        lora_adapter:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
          isOptional: true
      parameters:
        add_bos_token:
          defaultValue: true
          isOptional: true
          parameterType: BOOLEAN
        batch_size:
          defaultValue: 1.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        dtype:
          defaultValue: auto
          isOptional: true
          parameterType: STRING
        gpu_memory_utilization:
          defaultValue: 0.8
          isOptional: true
          parameterType: NUMBER_DOUBLE
        include_classification_tasks:
          defaultValue: true
          isOptional: true
          parameterType: BOOLEAN
        include_summarization_tasks:
          defaultValue: true
          isOptional: true
          parameterType: BOOLEAN
        limit:
          isOptional: true
          parameterType: NUMBER_INTEGER
        log_prompts:
          defaultValue: true
          isOptional: true
          parameterType: BOOLEAN
        max_batch_size:
          isOptional: true
          parameterType: NUMBER_INTEGER
        max_model_len:
          defaultValue: 4096.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        model_path:
          parameterType: STRING
        verbosity:
          defaultValue: INFO
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      artifacts:
        output_metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        output_prompts:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        output_results:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-evaluate-yoda-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - evaluate_yoda_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'transformers'\
          \ 'torch' 'accelerate' 'lm-eval[vllm]' 'unitxt' 'sacrebleu' 'datasets' \
          \ &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.15.2'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef evaluate_yoda_model(\n        model_path: str,\n        output_metrics:\
          \ dsl.Output[dsl.Metrics],\n        output_results: dsl.Output[dsl.Artifact],\n\
          \        output_prompts: dsl.Output[dsl.Artifact],\n        lora_adapter:\
          \ dsl.Input[dsl.Model] = None,\n        batch_size: int = 1,\n        limit:\
          \ int = None,\n        max_model_len: int = 4096,\n        gpu_memory_utilization:\
          \ float = 0.8,\n        dtype: str = \"auto\",\n        add_bos_token: bool\
          \ = True,\n        include_classification_tasks: bool = True,\n        include_summarization_tasks:\
          \ bool = True,\n        custom_translation_dataset: dsl.Input[dsl.Dataset]\
          \ = None,\n        log_prompts: bool = True,\n        verbosity: str = \"\
          INFO\",\n        max_batch_size: int = None,\n):\n    import logging\n \
          \   import os\n    import json\n    import time\n    import random\n   \
          \ from typing import Dict, Any, Optional\n\n    from lm_eval.tasks.unitxt\
          \ import task\n    from lm_eval.api.registry import get_model\n    from\
          \ lm_eval.api.model import LM\n    from lm_eval.evaluator import evaluate\n\
          \    from lm_eval.tasks import get_task_dict\n    from lm_eval.api.instance\
          \ import Instance\n    from lm_eval import tasks\n    from lm_eval.api.task\
          \ import TaskConfig\n    from lm_eval.api.metrics import mean\n    from\
          \ datasets import load_from_disk\n    import torch\n    import sacrebleu\n\
          \n    class TranslationTask(tasks.Task):\n        \"\"\"\n        A custom\
          \ lm-eval task for translation, using the greedy_until method\n        and\
          \ evaluating with the BLEU metric.\n        \"\"\"\n\n        VERSION =\
          \ 0\n\n        def __init__(self, dataset_path, task_name: str, log_prompts=False,\
          \ prompts_log=None):\n            self.dataset_path = dataset_path\n   \
          \         self.task_name = task_name\n            self.log_prompts = log_prompts\n\
          \            self.prompts_log = [] if prompts_log is None else prompts_log\n\
          \            config = TaskConfig(task=task_name, dataset_path=dataset_path)\n\
          \            super().__init__(config=config)\n            self.config.task\
          \ = task_name\n            self.fewshot_rnd = random.Random()\n\n      \
          \  def download(\n                self, data_dir=None, cache_dir=None, download_mode=None,\
          \ **kwargs\n        ) -> None:\n            self.dataset = {\"test\": load_from_disk(self.dataset_path)}\n\
          \n        def has_test_docs(self):\n            return \"test\" in self.dataset\n\
          \n        def has_validation_docs(self):\n            return False\n\n \
          \       def has_training_docs(self):\n            return False\n\n     \
          \   def test_docs(self):\n            return self.dataset[\"test\"]\n\n\
          \        def doc_to_text(self, doc):\n            return doc[\"prompt\"\
          ]\n\n        def doc_to_target(self, doc):\n            return doc[\"completion\"\
          ]\n\n        def construct_requests(self, doc, ctx, **kwargs):\n       \
          \     kwargs.pop(\"apply_chat_template\", False)\n            kwargs.pop(\"\
          chat_template\", False)\n            return Instance(\n                request_type=\"\
          generate_until\",\n                doc=doc,\n                arguments=(ctx,\
          \ {}),\n                idx=0,\n                **kwargs,\n            )\n\
          \n        def process_results(self, doc, results):\n            (generated_text,)\
          \ = results\n\n            prediction = generated_text.strip()\n\n     \
          \       if self.log_prompts:\n                try:\n                   \
          \ self.prompts_log.append(\n                        {\"prompt\": self.doc_to_text(doc),\
          \ \"response\": prediction}\n                    )\n                except\
          \ Exception:\n                    # Best-effort logging; avoid breaking\
          \ evaluation if logging fails\n                    pass\n\n            predictions\
          \ = [prediction]\n            references = [[self.doc_to_target(doc).strip()]]\n\
          \n            bleu_score = sacrebleu.corpus_bleu(predictions, references).score\n\
          \n            exact_match = 1.0 if prediction == references[0][0] else 0.0\n\
          \n            return {\"bleu\": bleu_score, \"exact_match\": exact_match}\n\
          \n        def aggregation(self):\n            return {\"bleu\": mean, \"\
          exact_match\": mean}\n\n        def should_decontaminate(self):\n      \
          \      return False\n\n        def doc_to_prefix(self, doc):\n         \
          \   return \"\"\n\n        def higher_is_better(self):\n            return\
          \ {\"bleu\": True, \"exact_match\": True}\n\n    TASK_CONFIGS = {\n    \
          \    \"classification\": [\n            {\n                \"task\": \"\
          classification_rte_simple\",\n                \"recipe\": \"card=cards.rte,template=templates.classification.multi_class.relation.simple\"\
          ,\n                \"group\": \"classification\",\n                \"output_type\"\
          : \"generate_until\",\n            },\n            {\n                \"\
          task\": \"classification_rte_default\",\n                \"recipe\": \"\
          card=cards.rte,template=templates.classification.multi_class.relation.default\"\
          ,\n                \"group\": \"classification\",\n                \"output_type\"\
          : \"generate_until\",\n            },\n            {\n                \"\
          task\": \"classification_rte_wnli\",\n                \"recipe\": \"card=cards.wnli,template=templates.classification.multi_class.relation.simple\"\
          ,\n                \"group\": \"classification\",\n                \"output_type\"\
          : \"generate_until\",\n            },\n        ],\n        \"summarization\"\
          : [\n            {\n                \"task\": \"summarization_xsum_formal\"\
          ,\n                \"recipe\": \"card=cards.xsum,template=templates.summarization.abstractive.formal,num_demos=0\"\
          ,\n                \"group\": \"summarization\",\n                \"output_type\"\
          : \"generate_until\",\n            }\n        ],\n    }\n\n    logging.basicConfig(\n\
          \        level=getattr(logging, verbosity.upper()),\n        format=\"%(asctime)s\
          \ - %(name)s - %(levelname)s - %(message)s\",\n    )\n    logger = logging.getLogger(__name__)\n\
          \n    os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"\n    os.environ[\"\
          HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n\n    logger.info(\"Validating\
          \ parameters...\")\n\n    if not torch.cuda.is_available():\n        raise\
          \ ValueError(\"CUDA is not available\")\n\n    if not (0.0 <= gpu_memory_utilization\
          \ <= 1.0):\n        raise ValueError(\"gpu_memory_utilization must be between\
          \ 0.0 and 1.0\")\n\n    if batch_size <= 0:\n        raise ValueError(\"\
          batch_size must be positive\")\n\n    if max_model_len <= 0:\n        raise\
          \ ValueError(\"max_model_len must be positive\")\n\n    if limit is not\
          \ None and limit <= 0:\n        raise ValueError(\"limit must be positive\
          \ or None\")\n\n    if (\n            not include_classification_tasks\n\
          \            and not include_summarization_tasks\n            and not custom_translation_dataset\n\
          \    ):\n        raise ValueError(\n            \"At least one of include_classification_tasks,\
          \ include_summarization_tasks, or custom_translation_dataset must be provided\"\
          \n        )\n\n    logger.info(\"Parameter validation passed\")\n\n    logger.info(\"\
          Creating tasks...\")\n    start_time = time.time()\n\n    eval_tasks = []\n\
          \    prompt_response_log = []\n\n    if custom_translation_dataset:\n  \
          \      logger.info(\"Adding custom translation task...\")\n        translation_task\
          \ = TranslationTask(\n            custom_translation_dataset.path,\n   \
          \         \"custom_translation\",\n            log_prompts=log_prompts,\n\
          \            prompts_log=prompt_response_log,\n        )\n        eval_tasks.append(translation_task)\n\
          \n    if include_classification_tasks:\n        logger.info(\"Adding classification\
          \ tasks...\")\n        classification_configs = TASK_CONFIGS[\"classification\"\
          ]\n\n        for config in classification_configs:\n            task_obj\
          \ = task.Unitxt(config=config)\n            # TODO: Remove after https://github.com/EleutherAI/lm-evaluation-harness/pull/3225\
          \ is merged.\n            task_obj.config.task = config[\"task\"]\n    \
          \        eval_tasks.append(task_obj)\n\n    if include_summarization_tasks:\n\
          \        logger.info(\"Adding summarization tasks...\")\n        summarization_config\
          \ = TASK_CONFIGS[\"summarization\"][0]\n\n        task_obj = task.Unitxt(config=summarization_config)\n\
          \        # TODO: Remove after https://github.com/EleutherAI/lm-evaluation-harness/pull/3225\
          \ is merged.\n        task_obj.config.task = summarization_config[\"task\"\
          ]\n        eval_tasks.append(task_obj)\n\n    task_dict = get_task_dict(eval_tasks)\n\
          \    logger.info(f\"Created {len(eval_tasks)} tasks in {time.time() - start_time:.2f}s\"\
          )\n\n    logger.info(\"Loading model...\")\n    start_time = time.time()\n\
          \n    try:\n        model_args = {\n            \"add_bos_token\": add_bos_token,\n\
          \            \"dtype\": dtype,\n            \"max_model_len\": max_model_len,\n\
          \            \"gpu_memory_utilization\": gpu_memory_utilization,\n     \
          \       \"pretrained\": model_path,\n            \"trust_remote_code\":\
          \ True,\n        }\n\n        # Optionally provide LoRA adapter to lm-eval's\
          \ VLLM backend\n        # The backend expects `lora_local_path` and internally\
          \ constructs the LoRARequest.\n        if lora_adapter and lora_adapter.path:\n\
          \            logger.info(\"LoRA adapter provided; passing lora_local_path\
          \ to VLLM backend\")\n            model_args[\"lora_local_path\"] = lora_adapter.path\n\
          \n        model_class = get_model(\"vllm\")\n        additional_config =\
          \ {\n            \"batch_size\": batch_size,\n            \"max_batch_size\"\
          : max_batch_size,\n            \"device\": None,\n        }\n\n        loaded_model\
          \ = model_class.create_from_arg_obj(model_args, additional_config)\n   \
          \     logger.info(f\"Model loaded successfully in {time.time() - start_time:.2f}s\"\
          )\n    except Exception as e:\n        logger.error(f\"Failed to load model:\
          \ {e}\")\n        raise RuntimeError(f\"Model loading failed: {e}\")\n\n\
          \    logger.info(\"Starting evaluation...\")\n    start_time = time.time()\n\
          \n    results = evaluate(\n        lm=loaded_model,\n        task_dict=task_dict,\n\
          \        limit=limit,\n        verbosity=verbosity,\n    )\n\n    logger.info(f\"\
          Evaluation completed in {time.time() - start_time:.2f}s\")\n\n    logger.info(\"\
          Saving results...\")\n\n    def clean_for_json(obj):\n        \"\"\"Recursively\
          \ clean objects to make them JSON serializable.\"\"\"\n        if isinstance(obj,\
          \ dict):\n            return {k: clean_for_json(v) for k, v in obj.items()}\n\
          \        elif isinstance(obj, list):\n            return [clean_for_json(item)\
          \ for item in obj]\n        elif isinstance(obj, (int, float, str, bool,\
          \ type(None))):\n            return obj\n        else:\n            # Convert\
          \ non-serializable objects to string representation\n            return\
          \ str(obj)\n\n    clean_results = clean_for_json(results)\n\n    output_results.name\
          \ = \"results.json\"\n\n    with open(output_results.path, \"w\") as f:\n\
          \        json.dump(clean_results, f, indent=2)\n    logger.info(f\"Results\
          \ saved to {output_results.path}\")\n\n    # Save prompt/response log for\
          \ custom TranslationTask only\n    if log_prompts and custom_translation_dataset\
          \ and len(prompt_response_log) > 0:\n        try:\n            output_prompts.name\
          \ = \"prompts.json\"\n            with open(output_prompts.path, \"w\")\
          \ as f:\n                json.dump(prompt_response_log, f, indent=2)\n \
          \           logger.info(f\"Prompt/response log saved to {output_prompts.path}\"\
          )\n        except Exception as e:\n            logger.warning(f\"Failed\
          \ to save prompt/response log: {e}\")\n\n    logger.info(\"Logging metrics...\"\
          )\n\n    for task_name, task_results in clean_results[\"results\"].items():\n\
          \        for metric_name, metric_value in task_results.items():\n      \
          \      if isinstance(metric_value, (int, float)):\n                # Skip\
          \ metrics that are 0 due to a bug in the RHOAI UI.\n                # TODO:\
          \ Fix RHOAI UI to handle 0 values.\n                # TODO: Ignore store_session_info\
          \ from metrics in RHOAI UI.\n                if metric_value == 0:\n   \
          \                 continue\n\n                metric_key = f\"{task_name}_{metric_name}\"\
          \n                output_metrics.log_metric(metric_key, metric_value)\n\
          \                logger.debug(f\"Logged metric: {metric_key} = {metric_value}\"\
          )\n\n    logger.info(\"Metrics logged successfully\")\n\n    logger.info(\"\
          Pipeline completed successfully\")\n\n"
        image: registry.access.redhat.com/ubi9/python-311:latest
pipelineInfo:
  name: evaluate-yoda-model
root:
  dag:
    outputs:
      artifacts:
        output_metrics:
          artifactSelectors:
          - outputArtifactKey: output_metrics
            producerSubtask: evaluate-yoda-model
        output_prompts:
          artifactSelectors:
          - outputArtifactKey: output_prompts
            producerSubtask: evaluate-yoda-model
        output_results:
          artifactSelectors:
          - outputArtifactKey: output_results
            producerSubtask: evaluate-yoda-model
    tasks:
      evaluate-yoda-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-evaluate-yoda-model
        inputs:
          artifacts:
            custom_translation_dataset:
              componentInputArtifact: custom_translation_dataset
            lora_adapter:
              componentInputArtifact: lora_adapter
          parameters:
            add_bos_token:
              componentInputParameter: add_bos_token
            batch_size:
              componentInputParameter: batch_size
            dtype:
              componentInputParameter: dtype
            gpu_memory_utilization:
              componentInputParameter: gpu_memory_utilization
            include_classification_tasks:
              componentInputParameter: include_classification_tasks
            include_summarization_tasks:
              componentInputParameter: include_summarization_tasks
            limit:
              componentInputParameter: limit
            log_prompts:
              componentInputParameter: log_prompts
            max_batch_size:
              componentInputParameter: max_batch_size
            max_model_len:
              componentInputParameter: max_model_len
            model_path:
              componentInputParameter: model_path
            verbosity:
              componentInputParameter: verbosity
        taskInfo:
          name: evaluate-yoda-model
  inputDefinitions:
    artifacts:
      custom_translation_dataset:
        artifactType:
          schemaTitle: system.Dataset
          schemaVersion: 0.0.1
        isOptional: true
      lora_adapter:
        artifactType:
          schemaTitle: system.Model
          schemaVersion: 0.0.1
        isOptional: true
    parameters:
      add_bos_token:
        defaultValue: true
        isOptional: true
        parameterType: BOOLEAN
      batch_size:
        defaultValue: 1.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      dtype:
        defaultValue: auto
        isOptional: true
        parameterType: STRING
      gpu_memory_utilization:
        defaultValue: 0.8
        isOptional: true
        parameterType: NUMBER_DOUBLE
      include_classification_tasks:
        defaultValue: true
        isOptional: true
        parameterType: BOOLEAN
      include_summarization_tasks:
        defaultValue: true
        isOptional: true
        parameterType: BOOLEAN
      limit:
        isOptional: true
        parameterType: NUMBER_INTEGER
      log_prompts:
        defaultValue: true
        isOptional: true
        parameterType: BOOLEAN
      max_batch_size:
        isOptional: true
        parameterType: NUMBER_INTEGER
      max_model_len:
        defaultValue: 4096.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      model_path:
        parameterType: STRING
      verbosity:
        defaultValue: INFO
        isOptional: true
        parameterType: STRING
  outputDefinitions:
    artifacts:
      output_metrics:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
      output_prompts:
        artifactType:
          schemaTitle: system.Artifact
          schemaVersion: 0.0.1
      output_results:
        artifactType:
          schemaTitle: system.Artifact
          schemaVersion: 0.0.1
schemaVersion: 2.1.0
sdkVersion: kfp-2.15.2
